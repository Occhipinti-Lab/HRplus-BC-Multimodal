{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23fa498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f44161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUNE_MODE: ON | Optuna Trials: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    matthews_corrcoef, brier_score_loss\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "# Global Configuration\n",
    "base_dir = Path().resolve()\n",
    "results_dir = base_dir.parent / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "TUNE_MODE = True        # Enable Optuna tuning\n",
    "N_TRIALS = 20\n",
    "SEED = 42\n",
    "\n",
    "print(f\"TUNE_MODE: {'ON' if TUNE_MODE else 'OFF'} | Optuna Trials: {N_TRIALS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2f4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Number of Feature for Each Modality\n",
    "tr_n = 250\n",
    "fl_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34842e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function for each model\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Compute all metrics including Brier and MCC.\"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob > 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = model.predict(X_test)\n",
    "        y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    brier = brier_score_loss(y_test, y_prob)\n",
    "\n",
    "    print(f\"✅ {model_name}: acc={acc:.3f}, f1={f1:.3f}, auc={auc:.3f}, mcc={mcc:.3f}, brier={brier:.3f}\")\n",
    "    return acc, prec, rec, f1, auc, mcc, brier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbed63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Transcriptomics and Fluxomics data\n",
    "def load_transcriptomic(base_dir, meta):\n",
    "    trans = pd.read_csv(base_dir.parent / \"dataset/csv/transcriptomic_hvg.csv\", index_col=0)\n",
    "    trans = pd.merge(trans, meta[[\"response\"]], left_index=True, right_index=True, how=\"inner\")\n",
    "    trans = trans.loc[:, ~trans.columns.str.upper().str.startswith(\"MT-\")]\n",
    "    trans[\"response\"] = trans[\"response\"].map({\"Responder\": 1, \"Non-responder\": 0})\n",
    "    y = trans[\"response\"].astype(np.int8)\n",
    "    X = trans.drop(columns=[\"response\"]).astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_fluxomic(base_dir):\n",
    "    flux = pd.read_csv(base_dir.parent / \"dataset/csv/fluxomics.csv\", index_col=0)\n",
    "    rxns = [c for c in flux.columns if c != \"response\" and not c.startswith(\"EX_\")]\n",
    "    flux = flux[rxns + [\"response\"]]\n",
    "    flux[\"response\"] = flux[\"response\"].map({\"Responder\": 1, \"Non-responder\": 0})\n",
    "    y = flux[\"response\"].astype(np.int8)\n",
    "    X = flux.drop(columns=[\"response\"]).astype(np.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865673f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and selected.\n"
     ]
    }
   ],
   "source": [
    "# Load Meta Data and select features\n",
    "meta = pd.read_csv(base_dir.parent / \"dataset/csv/metadata.csv\", index_col=0)\n",
    "hvg_genes = pd.read_csv(base_dir.parent / \"dataset/csv/hvg_genes.csv\", header=None).squeeze().tolist()\n",
    "\n",
    "X_trans, y_trans = load_transcriptomic(base_dir, meta)\n",
    "X_flux, y_flux = load_fluxomic(base_dir)\n",
    "\n",
    "# select top features\n",
    "tr_top = [g for g in hvg_genes if g in X_trans.columns][:tr_n]\n",
    "fl_top = X_flux.var().sort_values(ascending=False).index[:fl_n]\n",
    "\n",
    "X_tr_250 = X_trans[tr_top]\n",
    "X_fl_200 = X_flux[fl_top]\n",
    "\n",
    "# multimodal merge\n",
    "X_multi_full = pd.merge(X_trans, X_flux, left_index=True, right_index=True, how=\"inner\")\n",
    "y_multi = meta.loc[X_multi_full.index, \"response\"].map({\"Responder\": 1, \"Non-responder\": 0}).astype(np.int8)\n",
    "X_multi = X_multi_full[list(tr_top) + list(fl_top)]\n",
    "print(\"Data loaded and selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1db2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Objective Functions with class balance\n",
    "def class_balance(y):\n",
    "    neg, pos = (y == 0).sum(), (y == 1).sum()\n",
    "    return neg / pos if pos > 0 else 1.0\n",
    "\n",
    "\n",
    "def tune_logreg(trial, X_train, X_val, y_train, y_val):\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 100, log=True)\n",
    "    w = class_balance(y_train)\n",
    "    model = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=C, class_weight={0: 1, 1: w})\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    return roc_auc_score(y_val, preds)\n",
    "\n",
    "\n",
    "def tune_rf(trial, X_train, X_val, y_train, y_val):\n",
    "    w = class_balance(y_train)\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800, step=200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 16),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 6),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "        \"class_weight\": {0: 1, 1: w}\n",
    "    }\n",
    "    model = RandomForestClassifier(random_state=SEED, n_jobs=-1, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    return roc_auc_score(y_val, preds)\n",
    "\n",
    "\n",
    "def tune_xgb(trial, X_train, X_val, y_train, y_val):\n",
    "    w = class_balance(y_train)\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"scale_pos_weight\": w,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000, step=200)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params, n_jobs=-1, random_state=SEED, verbosity=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    return roc_auc_score(y_val, preds)\n",
    "\n",
    "\n",
    "def tune_ann(trial, X_train, X_val, y_train, y_val):\n",
    "    w = class_balance(y_train)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
    "    n1 = trial.suggest_int(\"n1\", 32, 128, step=32)\n",
    "    n2 = trial.suggest_int(\"n2\", 16, 64, step=16)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(n1, activation=\"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(n2, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    sample_weights = np.where(y_train == 1, w, 1)\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=30, batch_size=32, verbose=0, callbacks=[early])\n",
    "    preds = model.predict(X_val)\n",
    "    return roc_auc_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c5659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Models (Logistic Regression, Random Forest, XGBoost and ANN)\n",
    "def build_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Build all models, using class weights (balanced) for both tuned and untuned cases.\n",
    "    \"\"\"\n",
    "    w = class_balance(y_train)   # Compute class ratio\n",
    "    print(f\"Class balance ratio: neg/pos = {w:.2f}\")\n",
    "\n",
    "    if not TUNE_MODE:\n",
    "        # -------------------------\n",
    "        # Default models (no Optuna)\n",
    "        # -------------------------\n",
    "        models_out = {\n",
    "            \"LogReg\": (\n",
    "                LogisticRegression(\n",
    "                    C=0.5, solver=\"lbfgs\", max_iter=1000, class_weight={0: 1, 1: w}\n",
    "                ),\n",
    "                {\"C\": 0.5, \"class_weight\": {0: 1, 1: w}}\n",
    "            ),\n",
    "            \"RandomForest\": (\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=500, max_depth=10,\n",
    "                    class_weight={0: 1, 1: w},\n",
    "                    n_jobs=-1, random_state=SEED\n",
    "                ),\n",
    "                {\"n_estimators\": 500, \"max_depth\": 10, \"class_weight\": {0: 1, 1: w}}\n",
    "            ),\n",
    "            \"XGBoost\": (\n",
    "                xgb.XGBClassifier(\n",
    "                    learning_rate=0.05, max_depth=8, n_estimators=800,\n",
    "                    subsample=0.9, colsample_bytree=0.8,\n",
    "                    scale_pos_weight=w, n_jobs=-1, verbosity=0, random_state=SEED\n",
    "                ),\n",
    "                {\n",
    "                    \"learning_rate\": 0.05,\n",
    "                    \"max_depth\": 8,\n",
    "                    \"n_estimators\": 800,\n",
    "                    \"scale_pos_weight\": w\n",
    "                }\n",
    "            ),\n",
    "            # ANN placeholder — weights applied during training\n",
    "            \"ANN\": (None, {\"lr\": 0.001, \"dropout\": 0.5, \"n1\": 64, \"n2\": 32})\n",
    "        }\n",
    "        return models_out\n",
    "\n",
    "    else:\n",
    "        # -------------------------\n",
    "        # Optuna tuning case\n",
    "        # -------------------------\n",
    "        X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=SEED)\n",
    "        print(\"Running Optuna tuning for all models...\")\n",
    "        models_out = {}\n",
    "\n",
    "        for name, tuner in zip(\n",
    "            [\"LogReg\", \"RandomForest\", \"XGBoost\", \"ANN\"],\n",
    "            [tune_logreg, tune_rf, tune_xgb, tune_ann]\n",
    "        ):\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(lambda trial: tuner(trial, X_t, X_v, y_t, y_v), n_trials=N_TRIALS, show_progress_bar=True)\n",
    "            print(f\"✅ {name} best AUC={study.best_value:.3f} | params={study.best_params}\")\n",
    "\n",
    "            if name == \"LogReg\":\n",
    "                model = LogisticRegression(\n",
    "                    **study.best_params, solver=\"lbfgs\", max_iter=1000, class_weight={0: 1, 1: w}\n",
    "                )\n",
    "            elif name == \"RandomForest\":\n",
    "                model = RandomForestClassifier(**study.best_params, random_state=SEED, n_jobs=-1, class_weight={0: 1, 1: w})\n",
    "            elif name == \"XGBoost\":\n",
    "                model = xgb.XGBClassifier(**study.best_params, n_jobs=-1, random_state=SEED, verbosity=0, scale_pos_weight=w)\n",
    "            else:\n",
    "                model = None  # ANN uses params separately\n",
    "\n",
    "            models_out[name] = (model, study.best_params)\n",
    "\n",
    "        return models_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02940efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Loop\n",
    "results = []\n",
    "modalities = {\n",
    "    \"Transcriptomic (250)\": (X_tr_250, y_trans),\n",
    "    \"Fluxomic (200)\": (X_fl_200, y_flux),\n",
    "    \"Multimodal (250+200)\": (X_multi, y_multi)\n",
    "}\n",
    "\n",
    "for label, (X, y) in modalities.items():\n",
    "    print(f\"\\n==============================\\n{label}\\n==============================\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models_dict = build_models(X_train_scaled, y_train)\n",
    "\n",
    "    for model_name, (model, best_params) in models_dict.items():\n",
    "        print(f\"\\n Training {model_name} ...\")\n",
    "\n",
    "        if model_name == \"ANN\":\n",
    "            params = best_params if best_params else {\"lr\": 0.001, \"dropout\": 0.5, \"n1\": 64, \"n2\": 32}\n",
    "            w = class_balance(y_train)\n",
    "            ann = models.Sequential([\n",
    "                layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "                layers.Dense(params[\"n1\"], activation=\"relu\"),\n",
    "                layers.Dropout(params[\"dropout\"]),\n",
    "                layers.Dense(params[\"n2\"], activation=\"relu\"),\n",
    "                layers.Dense(1, activation=\"sigmoid\")\n",
    "            ])\n",
    "            ann.compile(optimizer=optimizers.Adam(learning_rate=params[\"lr\"]),\n",
    "                        loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "            early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "            sample_weights = np.where(y_train == 1, w, 1)\n",
    "            ann.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
    "                    epochs=30, batch_size=32, verbose=0,\n",
    "                    sample_weight=sample_weights, callbacks=[early_stop])\n",
    "            y_prob = ann.predict(X_test_scaled)\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred)\n",
    "            rec = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "            brier = brier_score_loss(y_test, y_prob)\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            acc, prec, rec, f1, auc, mcc, brier = evaluate_model(model, X_test_scaled, y_test, model_name)\n",
    "\n",
    "        result = {\n",
    "            \"Modality\": label,\n",
    "            \"Model\": model_name,\n",
    "            \"Acc\": acc, \"Prec\": prec, \"Rec\": rec, \"F1\": f1,\n",
    "            \"AUC\": auc, \"MCC\": mcc, \"Brier\": brier\n",
    "        }\n",
    "\n",
    "        if best_params:\n",
    "            result[\"Best_Params\"] = str(best_params)\n",
    "\n",
    "        # SHAP for multimodal XGBoost\n",
    "        if label == \"Multimodal (250+200)\" and model_name == \"XGBoost\":\n",
    "            print(\"Computing SHAP contributions...\")\n",
    "            sample_X = pd.DataFrame(X_test_scaled, columns=X.columns).sample(n=min(100, len(X_test)), random_state=42)\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_vals = explainer.shap_values(sample_X)\n",
    "            shap_df = pd.DataFrame({\n",
    "                \"Feature\": sample_X.columns,\n",
    "                \"MeanAbsSHAP\": np.abs(shap_vals).mean(axis=0)\n",
    "            })\n",
    "            shap_df[\"Modality\"] = [\"Fluxomic\" if f in X_flux.columns else \"Transcriptomic\" for f in shap_df[\"Feature\"]]\n",
    "            contrib = shap_df.groupby(\"Modality\")[\"MeanAbsSHAP\"].sum()\n",
    "            perc = 100 * contrib / contrib.sum()\n",
    "            result[\"SHAP_Tr_%\"] = perc.get(\"Transcriptomic\", 0)\n",
    "            result[\"SHAP_Fl_%\"] = perc.get(\"Fluxomic\", 0)\n",
    "            print(f\"SHAP → Tr: {perc.get('Transcriptomic',0):.1f}%, Fl: {perc.get('Fluxomic',0):.1f}%\")\n",
    "\n",
    "        results.append(result)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "file = results_dir / f\"model_results_optuna_extmetrics_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "results_df.to_csv(file, index=False)\n",
    "print(f\"\\n Results saved to {file}\")\n",
    "display(results_df.sort_values([\"Modality\", \"AUC\"], ascending=[True, False]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
